{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6tivfs5h6G1s",
        "outputId": "e85beb90-ac23-4e1a-ded3-61fb95654a6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (110 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.7/110.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions\n",
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqwWvVmPFCYj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "from datetime import timedelta\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import class_weight\n",
        "import contractions\n",
        "import spacy\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import (Embedding, Conv1D, GlobalMaxPooling1D,\n",
        "                                     Dense, Dropout, Input, Concatenate)\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import en_core_web_sm\n",
        "import nltk\n",
        "import warnings\n",
        "\n",
        "# Suppress warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Load necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = en_core_web_sm.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AijaIR2QFCuT"
      },
      "outputs": [],
      "source": [
        "def expand_contractions(text):\n",
        "    \"\"\"\n",
        "    Expands contractions in the given text.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text containing contractions.\n",
        "\n",
        "    Returns:\n",
        "        str: Text with expanded contractions.\n",
        "    \"\"\"\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def advanced_preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocesses text by expanding contractions, removing mentions, URLs, hashtags,\n",
        "    and applying tokenization, lemmatization, and stopword removal using spaCy.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to preprocess.\n",
        "\n",
        "    Returns:\n",
        "        str: Preprocessed text.\n",
        "    \"\"\"\n",
        "    text = expand_contractions(text)\n",
        "    # Remove user mentions\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|URL', '', text)\n",
        "    text = re.sub(r'#', '', text)\n",
        "    doc = nlp(text.lower())\n",
        "    tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def load_and_preprocess_data(file_path):\n",
        "    \"\"\"\n",
        "    Loads and preprocesses text data from a CSV file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the CSV file containing the text data.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with the preprocessed text.\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(file_path)\n",
        "    data['clean_text'] = data['text'].apply(advanced_preprocess_text)\n",
        "    return data\n",
        "\n",
        "def prepare_data_for_cnn(data, tokenizer, label_encoder):\n",
        "    \"\"\"\n",
        "    Prepares data for input into a CNN by tokenizing and padding the text,\n",
        "    and encoding the labels.\n",
        "\n",
        "    Args:\n",
        "        data (pd.DataFrame): Data containing the text and labels.\n",
        "        tokenizer (Tokenizer): Tokenizer for converting text to sequences.\n",
        "        label_encoder (LabelEncoder): Encoder for transforming labels.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Tuple of tokenized and padded text (X) and encoded labels (y).\n",
        "    \"\"\"\n",
        "    sequences = tokenizer.texts_to_sequences(data['clean_text'])\n",
        "    X = pad_sequences(sequences, maxlen=max_len)\n",
        "    y = label_encoder.transform(data['labels'])\n",
        "    return X, y\n",
        "\n",
        "def load_glove_embeddings(tokenizer, embedding_dim=100):\n",
        "    \"\"\"\n",
        "    Loads pre-trained GloVe embeddings and creates an embedding matrix.\n",
        "\n",
        "    Args:\n",
        "        tokenizer (Tokenizer): Tokenizer with a word index.\n",
        "        embedding_dim (int): Dimensionality of the embedding vectors.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Embedding matrix with GloVe embeddings for the words in the tokenizer's vocabulary.\n",
        "    \"\"\"\n",
        "    embeddings_index = {}\n",
        "    # Download GloVe embeddings\n",
        "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    !unzip glove.6B.zip\n",
        "    with open('glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = ''.join(values[:-100])  # Handle words with spaces\n",
        "            coefs = np.asarray(values[-100:], dtype='float32')\n",
        "            embeddings_index[word] = coefs\n",
        "    embedding_matrix = np.zeros((len(tokenizer.word_index) + 1, embedding_dim))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "        if i >= max_words:\n",
        "            continue\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "\n",
        "def create_multi_channel_cnn_model(vocab_size, embedding_matrix, embedding_dim=100):\n",
        "    \"\"\"\n",
        "    Creates a multi-channel CNN model for text classification.\n",
        "\n",
        "    Args:\n",
        "        vocab_size (int): Vocabulary size.\n",
        "        embedding_matrix (np.ndarray): Embedding matrix with pre-trained embeddings.\n",
        "        embedding_dim (int): Dimensionality of the embedding vectors.\n",
        "\n",
        "    Returns:\n",
        "        Model: Compiled CNN model.\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=(max_len,))\n",
        "    embedding = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix],\n",
        "                          input_length=max_len, trainable=False)(inputs)\n",
        "\n",
        "    convs = []\n",
        "    filter_sizes = [3, 4, 5]\n",
        "    for size in filter_sizes:\n",
        "        conv = Conv1D(128, kernel_size=size, activation='relu')(embedding)\n",
        "        pool = GlobalMaxPooling1D()(conv)\n",
        "        convs.append(pool)\n",
        "\n",
        "    concat = Concatenate()(convs)\n",
        "    dropout = Dropout(0.5)(concat)\n",
        "    outputs = Dense(1, activation='sigmoid')(dropout)\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def train_and_evaluate_cnn(X_train, y_train, X_test, y_test, vocab_size, experiment_name):\n",
        "    \"\"\"\n",
        "    Trains and evaluates a multi-channel CNN model for text classification.\n",
        "\n",
        "    Args:\n",
        "        X_train (np.ndarray): Training data.\n",
        "        y_train (np.ndarray): Training labels.\n",
        "        X_test (np.ndarray): Test data.\n",
        "        y_test (np.ndarray): Test labels.\n",
        "        vocab_size (int): Vocabulary size for embedding.\n",
        "        experiment_name (str): Name of the experiment for logging purposes.\n",
        "    \"\"\"\n",
        "    print(f\"\\nRunning {experiment_name}\")\n",
        "\n",
        "    # Calculate class weights to handle class imbalance\n",
        "    class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "    class_weights = dict(enumerate(class_weights))\n",
        "\n",
        "    model = create_multi_channel_cnn_model(vocab_size, embedding_matrix)\n",
        "\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2)\n",
        "\n",
        "    start_time = time.time()\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=64, validation_split=0.2,\n",
        "                        class_weight=class_weights, callbacks=[early_stopping, reduce_lr], verbose=1)\n",
        "\n",
        "    total_elapsed_time = time.time() - start_time\n",
        "    print(f\"Training completed in {timedelta(seconds=total_elapsed_time)}\")\n",
        "\n",
        "    # Evaluate on test set\n",
        "    y_pred_probs = model.predict(X_test).flatten()\n",
        "    y_pred = (y_pred_probs > 0.5).astype(int)\n",
        "\n",
        "    print(\"\\nEvaluating model on test data:\")\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "    print(f\"AUC-ROC: {roc_auc_score(y_test, y_pred_probs):.4f}\")\n",
        "    print(f\"Classification report:\\n{classification_report(y_test, y_pred)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLaBc7zViMoo"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess datasets\n",
        "olid_train = load_and_preprocess_data('olid-train-small.csv')\n",
        "olid_test = load_and_preprocess_data('olid-test.csv')\n",
        "hasoc_train = load_and_preprocess_data('hasoc-train.csv')\n",
        "combined_train = pd.concat([olid_train, hasoc_train], ignore_index=True)\n",
        "\n",
        "max_words = 20000\n",
        "max_len = 150\n",
        "\n",
        "# Fit tokenizer on combined training data\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(combined_train['clean_text'])\n",
        "\n",
        "# Use a single LabelEncoder fitted on combined labels\n",
        "le = LabelEncoder()\n",
        "le.fit(combined_train['labels'])\n",
        "\n",
        "# Prepare training and test data\n",
        "X_olid_train, y_olid_train = prepare_data_for_cnn(olid_train, tokenizer, le)\n",
        "X_hasoc_train, y_hasoc_train = prepare_data_for_cnn(hasoc_train, tokenizer, le)\n",
        "X_test_olid, y_test_olid = prepare_data_for_cnn(olid_test, tokenizer, le)\n",
        "\n",
        "embedding_dim = 100\n",
        "embedding_matrix = load_glove_embeddings(tokenizer, embedding_dim)\n",
        "\n",
        "# In-Domain Experiment (Train on OLIDv1 and Test on OLIDv1)\n",
        "train_and_evaluate_cnn(X_olid_train, y_olid_train, X_test_olid, y_test_olid,\n",
        "                       len(tokenizer.word_index) + 1, \"In-Domain Experiment (Train on OLIDv1 and Test on OLIDv1)\")\n",
        "\n",
        "# Cross-Domain Experiment (Train on HASOC and Test on OLIDv1)\n",
        "train_and_evaluate_cnn(X_hasoc_train, y_hasoc_train, X_test_olid, y_test_olid,\n",
        "                       len(tokenizer.word_index) + 1, \"Cross-Domain Experiment (Train on HASOC and Test on OLIDv1)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import contractions\n",
        "import spacy\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "class Preprocessor:\n",
        "    def __init__(self, max_words=20000, max_len=150):\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "        self.max_words = max_words\n",
        "        self.max_len = max_len\n",
        "        self.tokenizer = Tokenizer(num_words=max_words)\n",
        "\n",
        "    def expand_contractions(self, text):\n",
        "        return contractions.fix(text)\n",
        "\n",
        "    def advanced_preprocess_text(self, text):\n",
        "        text = self.expand_contractions(text)\n",
        "        text = re.sub(r'@\\w+', '', text)\n",
        "        text = re.sub(r'http\\S+|www\\S+|URL', '', text)\n",
        "        text = re.sub(r'#', '', text)\n",
        "        doc = self.nlp(text.lower())\n",
        "        tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "    def add_custom_features(self, data):\n",
        "        data['text_length'] = data['text'].apply(len)\n",
        "        data['special_char_count'] = data['text'].apply(lambda x: sum([1 for char in x if char in \"!?.\"]))\n",
        "        data['caps_count'] = data['text'].apply(lambda x: sum([1 for char in x if char.isupper()]))\n",
        "        data['avg_word_length'] = data['text'].apply(lambda x: np.mean([len(word) for word in x.split()]) if len(x.split()) > 0 else 0)\n",
        "        data['unique_words_ratio'] = data['text'].apply(lambda x: len(set(x.split())) / len(x.split()) if len(x.split()) > 0 else 0)\n",
        "        return data\n",
        "\n",
        "    def preprocess_dataset(self, data, text_column='text', label_column='label', fit_tokenizer=True):\n",
        "        \"\"\"\n",
        "        Preprocesses the entire dataset, including text preprocessing and feature engineering.\n",
        "\n",
        "        Args:\n",
        "            data (pd.DataFrame): Input DataFrame containing text and label columns.\n",
        "            text_column (str): Name of the column containing the text data.\n",
        "            label_column (str): Name of the column containing the labels.\n",
        "            fit_tokenizer (bool): Whether to fit the tokenizer on the data.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (X, y) where X is the preprocessed feature matrix and y is the label vector.\n",
        "        \"\"\"\n",
        "        # Ensure the required columns exist\n",
        "        assert text_column in data.columns, f\"'{text_column}' column not found in the dataset\"\n",
        "        assert label_column in data.columns, f\"'{label_column}' column not found in the dataset\"\n",
        "\n",
        "        # Preprocess text\n",
        "        data['clean_text'] = data[text_column].apply(self.advanced_preprocess_text)\n",
        "\n",
        "        # Add custom features\n",
        "        data = self.add_custom_features(data)\n",
        "\n",
        "        # Tokenize and pad sequences\n",
        "        if fit_tokenizer:\n",
        "            self.tokenizer.fit_on_texts(data['clean_text'])\n",
        "        \n",
        "        sequences = self.tokenizer.texts_to_sequences(data['clean_text'])\n",
        "        X = pad_sequences(sequences, maxlen=self.max_len)\n",
        "\n",
        "        # Extract labels\n",
        "        y = data[label_column].values\n",
        "\n",
        "        return X, y\n",
        "\n",
        "    def get_custom_features(self, X):\n",
        "        return X[['text_length', 'special_char_count', 'caps_count', 'avg_word_length', 'unique_words_ratio']].values\n",
        "\n",
        "    def transform_new_data(self, new_data, text_column='text'):\n",
        "        \"\"\"\n",
        "        Transforms new data using the fitted preprocessor.\n",
        "\n",
        "        Args:\n",
        "            new_data (pd.DataFrame): New data to transform, containing a text column.\n",
        "            text_column (str): Name of the column containing the text data.\n",
        "\n",
        "        Returns:\n",
        "            np.array: Transformed feature matrix for the new data.\n",
        "        \"\"\"\n",
        "        assert text_column in new_data.columns, f\"'{text_column}' column not found in the dataset\"\n",
        "\n",
        "        # Preprocess text\n",
        "        new_data['clean_text'] = new_data[text_column].apply(self.advanced_preprocess_text)\n",
        "\n",
        "        # Add custom features\n",
        "        new_data = self.add_custom_features(new_data)\n",
        "\n",
        "        # Tokenize and pad sequences\n",
        "        sequences = self.tokenizer.texts_to_sequences(new_data['clean_text'])\n",
        "        X = pad_sequences(sequences, maxlen=self.max_len)\n",
        "\n",
        "        return X\n",
        "\n",
        "    def get_vocab_size(self):\n",
        "        return len(self.tokenizer.word_index) + 1\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Load your datasets\n",
        "    olid_train = pd.read_csv('olid-train-small.csv')\n",
        "    olid_test = pd.read_csv('olid-test.csv')\n",
        "    hasoc_train = pd.read_csv('hasoc-train.csv')\n",
        "\n",
        "    # Initialize the preprocessor\n",
        "    preprocessor = Preprocessor()\n",
        "\n",
        "    # Preprocess OLID-train-small dataset\n",
        "    X_olid_train, y_olid_train = preprocessor.preprocess_dataset(olid_train, text_column='tweet', label_column='subtask_a')\n",
        "\n",
        "    # Preprocess OLID-test dataset\n",
        "    X_olid_test, y_olid_test = preprocessor.preprocess_dataset(olid_test, text_column='tweet', label_column='subtask_a', fit_tokenizer=False)\n",
        "\n",
        "    # Preprocess HASOC-train dataset\n",
        "    X_hasoc_train, y_hasoc_train = preprocessor.preprocess_dataset(hasoc_train, text_column='text', label_column='task_1', fit_tokenizer=False)\n",
        "\n",
        "    print(\"OLID-train preprocessed shape:\", X_olid_train.shape)\n",
        "    print(\"OLID-test preprocessed shape:\", X_olid_test.shape)\n",
        "    print(\"HASOC-train preprocessed shape:\", X_hasoc_train.shape)\n",
        "    print(\"Vocabulary size:\", preprocessor.get_vocab_size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbWcIo6UoBSz"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
